{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from utils import *\n",
    "from model import *\n",
    "from train import *\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset yelp_review_full (/home/utsav/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09ea1cbdf234ecbabfd29d6a6f71952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataset format to torch for split : train\n",
      "Converting dataset format to torch for split : test\n"
     ]
    }
   ],
   "source": [
    "dataset = load_hf_dataset('yelp_review_full')\n",
    "dataset = prepare_datasets(dataset,0.5)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding pad_token to tokenizer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('GPT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0e8e6fa1624be0bfd6f6a5863c1aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/325000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d400aac4ce478fb86528e9166e3558",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenization(example):\n",
    "    tokenized_example = {}\n",
    "    tokenized_example['text'] = tokenizer.tokenize(example['text'])\n",
    "    return tokenized_example\n",
    "\n",
    "# Apply the tokenizer to all elements in the dataset and create a new dataset\n",
    "tokenized_dataset = dataset.map(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenized_dataset['train']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(train_data, vector_size=100, window=5, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45258"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45258, 100)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenization(example):\n",
    "    # return {'ids':tokenizer(example[\"text\"])['input_ids']}\n",
    "\n",
    "def tokenization(example):\n",
    "    example['text'] = \n",
    "    return {'tokenized_text':tokenizer.tokenize(example['text'])}\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenization, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(dataset['train'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train'][0]['ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('included')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.get_vocab()['Ä cakes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = [[tokenizer.get_vocab()[x] for x in tokenizer.tokenize(dataset['train'][i]['text'])] for i in range(len(dataset['train']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('GPT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x:\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "args = x()\n",
    "args.tokenizer_max_length = 64\n",
    "args.warmup_steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_embeddings = True\n",
    "freeze_pretrained_embeddings = True\n",
    "\n",
    "Model1 = ConsolidatedModelClass(\n",
    "        model_name='GPT2',\n",
    "        num_layers=6,\n",
    "        use_pretrained_embeddings=use_pretrained_embeddings,\n",
    "        freeze_pretrained_embeddings=freeze_pretrained_embeddings,\n",
    "        optimizer='AdamW',\n",
    "        lr=0.001,\n",
    "        tokenizer=tokenizer,\n",
    "        scheduler=True,\n",
    "        args=args,\n",
    "        device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model1.scheduler.get_last_lr()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model1.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_embeddings = True\n",
    "freeze_pretrained_embeddings = False\n",
    "\n",
    "Model2 = ConsolidatedModelClass(\n",
    "        model_name='GPT2',\n",
    "        num_layers=6,\n",
    "        use_pretrained_embeddings=use_pretrained_embeddings,\n",
    "        freeze_pretrained_embeddings=freeze_pretrained_embeddings,\n",
    "        optimizer='AdamW',\n",
    "        lr=0.001,\n",
    "        tokenizer=tokenizer,\n",
    "        scheduler=None,\n",
    "        args=args,\n",
    "        device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_embeddings = False\n",
    "freeze_pretrained_embeddings = False\n",
    "\n",
    "Model3 = ConsolidatedModelClass(\n",
    "        model_name='GPT2',\n",
    "        num_layers=6,\n",
    "        use_pretrained_embeddings=use_pretrained_embeddings,\n",
    "        freeze_pretrained_embeddings=freeze_pretrained_embeddings,\n",
    "        optimizer='AdamW',\n",
    "        lr=0.001,\n",
    "        tokenizer=tokenizer,\n",
    "        scheduler=None,\n",
    "        args=args,\n",
    "        device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_hf_dataset('yelp_review_full')\n",
    "dataset = prepare_datasets(dataset,0.01)   \n",
    "\n",
    "print(\"Length of training dataset:\", len(dataset['train']))\n",
    "print(\"Length of validation dataset:\", len(dataset['test']))\n",
    "\n",
    "# ----------------------------- Load dataloaders ----------------------------- #\n",
    "train_loader, val_loader, test_loader = prepare_dataloaders(dataset,batch_size=64,num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(200)):\n",
    "\n",
    "    train_models(Model1,train_loader,args)\n",
    "    evaluate_models(Model1,test_loader,args)\n",
    "\n",
    "    train_models(Model2,train_loader,args)\n",
    "    evaluate_models(Model2,test_loader,args)\n",
    "\n",
    "    train_models(Model3,train_loader,args)\n",
    "    evaluate_models(Model3,test_loader,args)\n",
    "\n",
    "    print(Model1.losses['train_loss'][-1])\n",
    "    print(Model1.losses['val_loss'][-1])\n",
    "\n",
    "    print(Model2.losses['train_loss'][-1])\n",
    "    print(Model2.losses['val_loss'][-1])\n",
    "\n",
    "    print(Model3.losses['train_loss'][-1])\n",
    "    print(Model3.losses['val_loss'][-1])\n",
    "\n",
    "    # # for model in models:\n",
    "    # train_loss = Model1.losses['train_loss'][-1]\n",
    "    # val_loss = Model1.losses['val_loss'][-1]\n",
    "\n",
    "    # print(f\"\\nhas train loss : {train_loss} \\n \\\n",
    "    #                         and test loss : {val_loss}\")\n",
    "\n",
    "    # wandb.log({\n",
    "    #     f\"train loss\" : train_loss,\n",
    "    #     f\"val loss\" : val_loss,\n",
    "    # },\n",
    "    # step = epoch\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('replearning2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ca15d85a21dc8447415e9277c881cdd22af3252960999305bb60d95159cfefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
