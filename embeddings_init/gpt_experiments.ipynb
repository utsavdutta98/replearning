{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from utils import *\n",
    "from model import *\n",
    "from train import *\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding pad_token to tokenizer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('GPT2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x:\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "args = x()\n",
    "args.tokenizer_max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_embeddings = True\n",
    "freeze_pretrained_embeddings = True\n",
    "\n",
    "Model1 = ConsolidatedModelClass(\n",
    "        model_name='GPT2',\n",
    "        num_layers=6,\n",
    "        use_pretrained_embeddings=use_pretrained_embeddings,\n",
    "        freeze_pretrained_embeddings=freeze_pretrained_embeddings,\n",
    "        optimizer='AdamW',\n",
    "        lr=0.001,\n",
    "        tokenizer=tokenizer,\n",
    "        scheduler=None,\n",
    "        args=args,\n",
    "        device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_embeddings = True\n",
    "freeze_pretrained_embeddings = False\n",
    "\n",
    "Model2 = ConsolidatedModelClass(\n",
    "        model_name='GPT2',\n",
    "        num_layers=6,\n",
    "        use_pretrained_embeddings=use_pretrained_embeddings,\n",
    "        freeze_pretrained_embeddings=freeze_pretrained_embeddings,\n",
    "        optimizer='AdamW',\n",
    "        lr=0.001,\n",
    "        tokenizer=tokenizer,\n",
    "        scheduler=None,\n",
    "        args=args,\n",
    "        device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pretrained_embeddings = False\n",
    "freeze_pretrained_embeddings = False\n",
    "\n",
    "Model3 = ConsolidatedModelClass(\n",
    "        model_name='GPT2',\n",
    "        num_layers=6,\n",
    "        use_pretrained_embeddings=use_pretrained_embeddings,\n",
    "        freeze_pretrained_embeddings=freeze_pretrained_embeddings,\n",
    "        optimizer='AdamW',\n",
    "        lr=0.001,\n",
    "        tokenizer=tokenizer,\n",
    "        scheduler=None,\n",
    "        args=args,\n",
    "        device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset yelp_review_full (/home/utsav/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e807bc20b54d47bdbc99091be41fb37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/utsav/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-b0c3fdb0a0b3a7bb.arrow\n",
      "Loading cached shuffled indices for dataset at /home/utsav/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-a0f1bfa2f136ab06.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting dataset format to torch for split : train\n",
      "Converting dataset format to torch for split : test\n",
      "Length of training dataset: 6500\n",
      "Length of validation dataset: 500\n"
     ]
    }
   ],
   "source": [
    "dataset = load_hf_dataset('yelp_review_full')\n",
    "dataset = prepare_datasets(dataset,0.01)   \n",
    "\n",
    "print(\"Length of training dataset:\", len(dataset['train']))\n",
    "print(\"Length of validation dataset:\", len(dataset['test']))\n",
    "\n",
    "# ----------------------------- Load dataloaders ----------------------------- #\n",
    "train_loader, val_loader, test_loader = prepare_dataloaders(dataset,batch_size=64,num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4802"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |   4121 MiB |  11791 MiB |  23891 GiB |  23887 GiB |\\n|       from large pool |   4101 MiB |  11763 MiB |  23851 GiB |  23847 GiB |\\n|       from small pool |     20 MiB |     33 MiB |     40 GiB |     40 GiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |   4121 MiB |  11791 MiB |  23891 GiB |  23887 GiB |\\n|       from large pool |   4101 MiB |  11763 MiB |  23851 GiB |  23847 GiB |\\n|       from small pool |     20 MiB |     33 MiB |     40 GiB |     40 GiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |   4078 MiB |  11744 MiB |  23866 GiB |  23862 GiB |\\n|       from large pool |   4057 MiB |  11716 MiB |  23826 GiB |  23822 GiB |\\n|       from small pool |     20 MiB |     33 MiB |     40 GiB |     40 GiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |   9652 MiB |  13460 MiB | 102746 MiB |  93094 MiB |\\n|       from large pool |   9618 MiB |  13424 MiB | 102694 MiB |  93076 MiB |\\n|       from small pool |     34 MiB |     36 MiB |     52 MiB |     18 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |   4396 MiB |   4639 MiB |  13176 GiB |  13172 GiB |\\n|       from large pool |   4390 MiB |   4630 MiB |  13136 GiB |  13131 GiB |\\n|       from small pool |      5 MiB |      8 MiB |     40 GiB |     40 GiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     952    |    1446    |    1154 K  |    1153 K  |\\n|       from large pool |     316    |     555    |     742 K  |     741 K  |\\n|       from small pool |     636    |     951    |     412 K  |     412 K  |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     952    |    1446    |    1154 K  |    1153 K  |\\n|       from large pool |     316    |     555    |     742 K  |     741 K  |\\n|       from small pool |     636    |     951    |     412 K  |     412 K  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     157    |     257    |     583    |     426    |\\n|       from large pool |     140    |     242    |     557    |     417    |\\n|       from small pool |      17    |      18    |      26    |       9    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     150    |     161    |  451285    |  451135    |\\n|       from large pool |     103    |     138    |  306604    |  306501    |\\n|       from small pool |      47    |      49    |  144681    |  144634    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16443310080, 50962169856)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.mem_get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat May  6 14:33:08 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Quadro RTX 8000     Off  | 00000000:15:00.0 Off |                  Off |\n",
      "| 33%   43C    P8    33W / 260W |  32919MiB / 48601MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Quadro RTX 8000     Off  | 00000000:2D:00.0 Off |                  Off |\n",
      "| 34%   53C    P8    36W / 260W |  46197MiB / 48598MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2060      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    967930      C   python3                         21105MiB |\n",
      "|    0   N/A  N/A   1127572      C   ...s/replearning2/bin/python    10451MiB |\n",
      "|    0   N/A  N/A   2786632      C   ...gbin/anaconda3/bin/python     1355MiB |\n",
      "|    1   N/A  N/A      2060      G   /usr/lib/xorg/Xorg                 27MiB |\n",
      "|    1   N/A  N/A      2145      G   /usr/bin/gnome-shell                5MiB |\n",
      "|    1   N/A  N/A   1030918      C   python3                         18725MiB |\n",
      "|    1   N/A  N/A   1047057      C   python3                          4977MiB |\n",
      "|    1   N/A  N/A   1047058      C   python3                          4977MiB |\n",
      "|    1   N/A  N/A   1047059      C   python3                          4977MiB |\n",
      "|    1   N/A  N/A   1165044      C   python3                          3105MiB |\n",
      "|    1   N/A  N/A   1165045      C   python3                          3105MiB |\n",
      "|    1   N/A  N/A   1165046      C   python3                          3105MiB |\n",
      "|    1   N/A  N/A   1169307      C   python3                          3187MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:31,  3.25it/s]00:00<?, ?it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.73it/s]\n",
      "102it [00:31,  3.28it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.87it/s]\n",
      "102it [00:31,  3.29it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.65it/s]\n",
      "  0%|          | 1/200 [01:42<5:40:35, 102.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.9978000977460075\n",
      "6.233442842960358\n",
      "7.21353894588994\n",
      "6.429744899272919\n",
      "6.517992230022655\n",
      "6.181263744831085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [00:31,  3.28it/s]\n",
      "100%|██████████| 8/8 [00:02<00:00,  3.90it/s]\n",
      "18it [00:05,  3.16it/s]\n",
      "  0%|          | 1/200 [02:23<7:55:41, 143.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m train_models(Model1,train_loader,args)\n\u001b[1;32m      4\u001b[0m evaluate_models(Model1,test_loader,args)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m evaluate_models(Model2,test_loader,args)\n\u001b[1;32m      9\u001b[0m train_models(Model3,train_loader,args)\n",
      "File \u001b[0;32m~/representation_learning/embeddings_init/train.py:22\u001b[0m, in \u001b[0;36mtrain_models\u001b[0;34m(Model, train_loader, args)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39m# iterate over dataloader\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mfor\u001b[39;00m i,batch \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(train_loader)):\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m     \u001b[39m# compute loss for each model and log\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     loss \u001b[39m=\u001b[39m Model\u001b[39m.\u001b[39;49mstep(batch)\n\u001b[1;32m     23\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     25\u001b[0m \u001b[39m# log in central dict, with average train loss\u001b[39;00m\n",
      "File \u001b[0;32m~/representation_learning/embeddings_init/model.py:131\u001b[0m, in \u001b[0;36mConsolidatedModelClass.step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    129\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(tokenized_batch)\n\u001b[1;32m    130\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m--> 131\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler_flag:\n",
      "File \u001b[0;32m~/anaconda3/envs/replearning2/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/replearning2/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(200)):\n",
    "\n",
    "    train_models(Model1,train_loader,args)\n",
    "    evaluate_models(Model1,test_loader,args)\n",
    "\n",
    "    train_models(Model2,train_loader,args)\n",
    "    evaluate_models(Model2,test_loader,args)\n",
    "\n",
    "    train_models(Model3,train_loader,args)\n",
    "    evaluate_models(Model3,test_loader,args)\n",
    "\n",
    "    print(Model1.losses['train_loss'][-1])\n",
    "    print(Model1.losses['val_loss'][-1])\n",
    "\n",
    "    print(Model2.losses['train_loss'][-1])\n",
    "    print(Model2.losses['val_loss'][-1])\n",
    "\n",
    "    print(Model3.losses['train_loss'][-1])\n",
    "    print(Model3.losses['val_loss'][-1])\n",
    "\n",
    "    # # for model in models:\n",
    "    # train_loss = Model1.losses['train_loss'][-1]\n",
    "    # val_loss = Model1.losses['val_loss'][-1]\n",
    "\n",
    "    # print(f\"\\nhas train loss : {train_loss} \\n \\\n",
    "    #                         and test loss : {val_loss}\")\n",
    "\n",
    "    # wandb.log({\n",
    "    #     f\"train loss\" : train_loss,\n",
    "    #     f\"val loss\" : val_loss,\n",
    "    # },\n",
    "    # step = epoch\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('replearning2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ca15d85a21dc8447415e9277c881cdd22af3252960999305bb60d95159cfefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
